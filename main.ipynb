{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "nervous-treasure",
   "metadata": {},
   "source": [
    "# electronic-gadget-identifier\n",
    "Machine learning project that identifies different electronic gadgets\n",
    "\n",
    "* Jesse BÃ¥tman (TT2018-3A) - e1700826\n",
    "* Joonatan Peltonen (TT2018-3B) - e1700807"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cleared-generator",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch import nn, optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continued-feeding",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_size = 256\n",
    "pretrained_means = [0.485, 0.456, 0.406]\n",
    "pretrained_stds= [0.229, 0.224, 0.225]\n",
    "\n",
    "transform = transforms.Compose([\n",
    "                                transforms.Resize(pretrained_size),\n",
    "                                transforms.CenterCrop(224),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean = pretrained_means, std = pretrained_stds)\n",
    "                              ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atomic-audience",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = datasets.ImageFolder(r'datasets\\training', transform=transform)\n",
    "validationset = datasets.ImageFolder(r'datasets\\validation', transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "raised-flush",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "validationloader = torch.utils.data.DataLoader(validationset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trained-vegetable",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deluxe-giving",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(images[1].permute(1, 2, 0).numpy().astype(np.uint8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polish-plenty",
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = plt.figure()\n",
    "num_of_images = 60\n",
    "for index in range(1, num_of_images + 1):\n",
    "    plt.subplot(6, 10, index)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(images[index].permute(1, 2, 0).numpy().astype(np.uint8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "awful-rendering",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "pretrained_model = models.vgg16_bn(pretrained = True)\n",
    "\n",
    "print(pretrained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tested-homeless",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_layer = nn.Sequential(nn.Linear(25088, 4096), \n",
    "                                     nn.ReLU(),\n",
    "                                     nn.Dropout(p=0.3),\n",
    "                                     nn.Linear(4096, 4),\n",
    "                                     nn.LogSoftmax(dim=1))\n",
    "\n",
    "pretrained_model.classifier =  classification_layer\n",
    "\n",
    "print(pretrained_model.classifier)\n",
    "model = pretrained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intelligent-measure",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "torch.set_num_threads(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "warming-qatar",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_conf_features(datasetloader):\n",
    "    conv_features = []\n",
    "    labels_list = []\n",
    "\n",
    "    for images, labels in datasetloader:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            # modeling for each image batch\n",
    "            conv_layer_output = model.features(images)        \n",
    "\n",
    "            # flatten output as the next layers are the fully connected layers\n",
    "            output = conv_layer_output.view(conv_layer_output.size(0), -1)\n",
    "\n",
    "            conv_features.extend(output.data.cpu().numpy())\n",
    "            labels_list.extend(labels.data.cpu().numpy())\n",
    "            \n",
    "    return (torch.Tensor(conv_features), torch.LongTensor(labels_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "casual-newspaper",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_features, labels_list = run_conf_features(trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surgical-lithuania",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_features_dataset = torch.utils.data.TensorDataset(conv_features, labels_list)\n",
    "trainloader = torch.utils.data.DataLoader(conv_features_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sticky-functionality",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_features, labels_list = run_conf_features(validationloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extraordinary-firmware",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_features_dataset = torch.utils.data.TensorDataset(conv_features, labels_list)\n",
    "validationloader = torch.utils.data.DataLoader(conv_features_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "private-feelings",
   "metadata": {},
   "outputs": [],
   "source": [
    "time0 = time()\n",
    "epochs = 3 # total number of iteration for training, training can be heavy, try first only one epoch.\n",
    "\n",
    "# exercise 3: For plotting the model quality metrics.\n",
    "epoch_list = []\n",
    "training_loss_list = []\n",
    "training_acc_list = []\n",
    "validation_loss_list = []\n",
    "validation_acc_list = []\n",
    "\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    # Variables to store the new quality metrics\n",
    "    training_acc = 0\n",
    "    validation_loss = 0\n",
    "    validation_acc = 0\n",
    "    for images, labels in trainloader:\n",
    "        model.train()\n",
    "        # CNN: flattening is not needed anymore, we expect 2d image.\n",
    "\n",
    "        # defining gradient in each epoch as 0\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # modeling for each image batch\n",
    "        #conv_layer_output = model.features(images)        \n",
    "        #conv_layer_output = model.avgpool(conv_layer_output)\n",
    "        \n",
    "        # remember to flatten before entering to the fully connected layer\n",
    "        #conv_layer_output = conv_layer_output.view(conv_layer_output.size(0), -1)\n",
    "        \n",
    "        output = model.classifier(images)\n",
    "        \n",
    "        # calculating the loss\n",
    "        loss = criterion(output, labels)\n",
    "        \n",
    "        # This is where the model learns by backpropagating\n",
    "        loss.backward()\n",
    "        \n",
    "        # And optimizes its weights here\n",
    "        optimizer.step()\n",
    "        \n",
    "        # calculating the loss\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        \n",
    "        # Exercise 3: calculate training accuracy\n",
    "        with torch.no_grad():\n",
    "            # Output of the network are log-probabilities, need to take exponential for probabilities\n",
    "            _, predictions = torch.exp(output).max(1)\n",
    "            # match predictions with true labels and count how many predictions are correct. Divide by batch size to get average for this batch.\n",
    "            training_acc += torch.sum(predictions == labels).item() / len(images)            \n",
    "\n",
    "    # Calculate validation loss and accuracy\n",
    "    for images, labels in validationloader:\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            # CNN: flattening is not needed anymore, we expect 2d image.\n",
    "\n",
    "            # modeling for each image batch\n",
    "            output = model.classifier(images)\n",
    "\n",
    "            # calculating the loss\n",
    "            loss = criterion(output, labels)\n",
    "\n",
    "            # calculating the loss\n",
    "            validation_loss += loss.item()\n",
    "\n",
    "            # Output of the network are log-probabilities, need to take exponential for probabilities\n",
    "            _, predictions = torch.exp(output).max(1)\n",
    "            # match predictions with true labels and count how many predictions are correct. Divide by batch size to get average for this batch.\n",
    "            validation_acc += torch.sum(predictions == labels).item() / len(images)            \n",
    "\n",
    "    print(\"Epoch {} - Elapsed minutes {} - Train loss: {:0.5f}  Train acc: {:0.5f} Val loss: {:0.5f} Val acc: {:0.5f}\".format(e, (time()-time0)/60,running_loss/len(trainloader),training_acc/len(trainloader), validation_loss/len(validationloader), validation_acc/len(validationloader)))\n",
    "    epoch_list.append(e);\n",
    "    training_loss_list.append(running_loss/len(trainloader))\n",
    "    training_acc_list.append(training_acc/len(trainloader))\n",
    "    validation_loss_list.append(validation_loss/len(validationloader))\n",
    "    validation_acc_list.append(validation_acc/len(validationloader))\n",
    "\n",
    "print(\"\\nTraining Time (in minutes) =\",(time()-time0)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aware-comment",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epoch_list, training_loss_list, 'g', label='Training loss')\n",
    "plt.plot(epoch_list, validation_loss_list, 'b', label='validation loss')\n",
    "plt.title('Training and Validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "allied-marking",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epoch_list, training_acc_list, 'g', label='Training accuracy')\n",
    "plt.plot(epoch_list, validation_acc_list, 'b', label='validation accuracy')\n",
    "plt.title('Training and Validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sitting-factor",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
